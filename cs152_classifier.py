# -*- coding: utf-8 -*-
"""CS152 Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1R5CQoYU191wKckBsyskltbyN8kTVqkGh
"""

!pip install nudenet
!pip install py-agender
!pip install hmtai
!pip install wget
!pip3 install tensorflow==1.10

# mounting google drive
from google.colab import drive 
drive.mount("/content/drive", force_remount=True)

from nudenet import NudeClassifier
from pyagender import PyAgender
import hmtai
#from tensorflow.keras import backend
import urllib.request
import wget
import tensorflow as tf
import keras
import cv2
import os

# classifies an image and finds the minimum age of all people present in the image
def age_class(filename):
  agender = PyAgender() 
  faces = agender.detect_genders_ages(cv2.imread(filename))
  min_age = 200
  for face in faces:
    curr_age = face['age']
    min_age = min(curr_age, min_age)
  print("The minimum age found was: " + str(min_age))
  return min_age

# classifies an image to contain nudity with a specific probability betwee 0 and 1
def nude_class(filename):
  classifier = NudeClassifier()
  nude_results = classifier.classify(filename)
  nude_prob = nude_results[filename]['unsafe']
  print("The probability that nudity is present in this image is: " + str(nude_prob))
  return nude_results[filename]['unsafe']

# determines whether an image is considered CSAM, return True if CSAM and False otherwise
def is_csam(filename):
  age = age_class(filename)
  nude_prob = nude_class(filename)  
  csam = nude_prob > 0.8 and age < 18
  return csam

# gets all the image paths of images in a given directory
def get_all_images(file_dir):
  data = []
  for filename in os.listdir(file_dir):
      if filename.endswith("jpg") or filename.endswith("png") or filename.endswith("jpeg"): 
          data.append(filename)
  print(str(len(data)) +  " images found in the directory")
  return data

# TESTING SINGLE RANDOM IMAGE
is_csam("/content/drive/Shareddrives/CS152/Image Dataset/Negative Images/image5.jpg")

# Negative Images
neg_images = get_all_images("/content/drive/Shareddrives/CS152/Image Dataset/Negative Images/")
pos_count = 0
for curr_im in neg_images:
  print("------------" + curr_im + "--------------")
  pos_label = is_csam("/content/drive/Shareddrives/CS152/Image Dataset/Negative Images/" + curr_im)
  if pos_label:
    pos_count += 1
    print("**LOOK** "+ curr_im + " was falsely detected to be CSAM")
print("A total of " + str(pos_count) + " images were falsely labelled")

# concat images
hentai_images = get_all_images("/content/drive/MyDrive/Hentai")
face_images = get_all_images("/content/drive/Shareddrives/CS152/Image Dataset/Kids Faces")
pos_count = 0
for i in range(len(hentai_images)):
  print("------------" + hentai_images[i] + "--------------")
  if nude_class("/content/drive/MyDrive/Hentai/" + hentai_images[i]) > 0.8 and age_class("/content/drive/Shareddrives/CS152/Image Dataset/Kids Faces/" + face_images[i]) <= 18:
    x = 5
  else:
    pos_count += 1
    print("**LOOK** "+  face_images[i] + " was falsely detected to not be CSAM")
print("A total of " + str(pos_count) + " images were falsely labelled")





